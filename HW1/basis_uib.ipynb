{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary\n",
    "\n",
    "Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data01 = np.load(\"data/data01.npy\")\n",
    "X01, Y01 = data01[:,0], data01[:,1]\n",
    "data02 = np.load(\"data/data02.npy\")\n",
    "X02, Y02 = data02[:,0], data02[:,1]\n",
    "data03 = np.load(\"data/data03.npy\")\n",
    "X03, Y03 = data03[:,0], data03[:,1]\n",
    "data04 = np.load(\"data/data04.npy\")\n",
    "X04, Y04 = data04[:,0], data04[:,1]\n",
    "\n",
    "def sort_wrt_x_axis(X,Y, return_idx=False):\n",
    "    \"\"\"\n",
    "    Sort X and Y with respect to X, usefull for better plots\n",
    "    \"\"\"\n",
    "    idx = np.argsort(X)\n",
    "    if return_idx:\n",
    "        res = (X[idx], Y[idx], idx)\n",
    "    else:\n",
    "        res = (X[idx], Y[idx])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "\n",
    "#### Idea\n",
    "\n",
    "Given a point cloud defined by $n$ pairs $(x_i,y_i)$ where $x_i$ are features (or predictors) and $y$ are observed values.\n",
    "The objective of linear regression is to find the straight line that fits the best to this point cloud, i.e find the line's parameters $W$ (for *weights*) such that $\\textrm{SSE} = \\sum_i \\epsilon_i^2$ is minimal in \n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{X} \\mathbf{w} + \\mathbf{\\epsilon}$$\n",
    "\n",
    "It turns out that this error is minimal for $$\\hat{\\mathbf{w}} = X^+\\mathbf{y}$$ where $X^+$ is the pseudo-inverse of $X$, i.e  $X^+ = (X'X)^{-1}X'$\n",
    "\n",
    "Once your parameter $\\hat{w}$ computed you can use your model on new feature values $X_{new}$ to predict $\\mathbf{y}_{pred}$ with $$\\mathbf{y}_{pred} = X_{new} \\hat{\\mathbf{w}}$$\n",
    "\n",
    "#### Notations for **multivariate**, **simple** linear regression\n",
    "\n",
    "- $n$ input vectors $\\mathbf{x}_{i=1..n}$ in a $p+1$ dimensional space representing $n$ samples of $p$ features (+ 1 dummy variable for the biais) , i.e \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{x}_i = \\begin{bmatrix}1, x_{i,1}, x_{i,2}, \\cdots, x_{i,p}\\end{bmatrix}^T \\in R^{p}  \\qquad \\textrm{and}  \\;\n",
    "\\mathbf{X} = \n",
    "\\begin{bmatrix}\n",
    "1 & x_{1,1} & x_{1,2} & \\cdots & x_{1,p} \\\\\n",
    "1 & x_{2,1} & x_{2,2} & \\cdots & x_{2,p} \\\\\n",
    "\\vdots & \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "1 & x_{n,1} & x_{n,2} & \\cdots & x_{n,p}\n",
    "\\end{bmatrix} \\in R^{(n, p+1)}\n",
    "\\end{equation}\n",
    "\n",
    "- $n$ output values $\\mathbf{y}_{i=1..n}$ (one for each input) representing $n$ observations in a $R$\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{y} = \n",
    "\\begin{bmatrix}y_{1}, y_{2}, \\cdots,y_{n}\n",
    "\\end{bmatrix}^T \\in R^{n}\n",
    "\\end{equation}\n",
    "\n",
    "- $p+1$ unknown parameters or regression coefficients $w_{k=0..p}$ (one for each feature + biais) representing the slope associated to the $k$th feature\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{w} = \n",
    "\\begin{bmatrix}w_{0}, w_{0}, \\cdots & w_{p} \\end{bmatrix}^T\\in R^{p+1}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dummy_variable(x):\n",
    "    return np.c_[np.ones(len(x)), x]\n",
    "\n",
    "def fit_LR(x,y, add_biais=True):\n",
    "    \"\"\"\n",
    "    Solve the linear regression problem, i.e compute W_hat\n",
    "    \"\"\"\n",
    "    X = np.copy(x)\n",
    "    # Add dummy variable if not already done\n",
    "    if add_biais:\n",
    "        X = add_dummy_variable(X)\n",
    "    # Returns W = pseudo_inv(X) * y\n",
    "    return np.matmul(np.linalg.pinv(X),y)\n",
    "\n",
    "def predict_LR(x, W, add_biais=True):\n",
    "    \"\"\"\n",
    "    Returns ``Y_pred`` given new features values and a weights\n",
    "    \"\"\"\n",
    "    X = np.copy(x)\n",
    "    # Add dummy variable if not already done\n",
    "    if add_biais:\n",
    "        X = add_dummy_variable(X)\n",
    "    # Returns y_pred = X * W\n",
    "    return np.matmul(X,W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Illustration: Univariate simple linear regression\n",
    "\n",
    "- If $p$ = 1 (i.e one feature), the regression is simple.\n",
    "- If $q$ = 1 (i.e predicted value in $R$), the regression is univariate\n",
    "\n",
    "Questions:\n",
    "\n",
    "1. Datasets \n",
    "  1. Using the function [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) from scikit-learn library, split ``X01`` and ``Y01`` into 3 datasets, a training one, a validation one and a testing one with a ratio 0.6, 0.2, 0.2.\n",
    "  2. Print the length (number of datapoints) of each datasets using [len()](https://docs.python.org/3/library/functions.html#len) or [ndarray.shape](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.shape.html)\n",
    "2. Fitting your model\n",
    "  1. Call ``fit_LR`` on your training dataset to compute the weights (store them in ``W``)\n",
    "  2. What do ``W[0]`` and ``W[1]`` represent?\n",
    "3. Predictions\n",
    "  1. Call  ``predict_LR`` on your validation dataset and store the output in ``Y01_pred``\n",
    "4. Same thing using scikit-learn: ``regr`` is an object of the class [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn-linear-model-linearregression) from the scikit-learn library\n",
    "  1. Apply the [fit()](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.fit) method to ``regr``  using your training dataset.\n",
    "  2. Apply the [predict()](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.predict) method to``regr`` using your validation dataset.\n",
    "  3. Take a look at how the mse (Mean Square Error) of ``Y01_pred`` is computed and then compute the mse of ``Y01_pred_scikit``\n",
    "  4. Take a look at how how the coefficient of determination $R^2$ is computed and then compute $R^2$ of ``Y01_pred_scikit`` ([Wikipedia](https://en.wikipedia.org/wiki/Coefficient_of_determination): \"*$R^2$ is the proportion of the variance in the dependent variable that is predictable from the independent variable(s).  It provides a measure of how well observed outcomes are replicated by the model, based on the proportion of total variation of outcomes explained by the model\"*)\n",
    "  5. Compare with the Mean Square Error and the coefficient of determination of ``Y01_pred``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" ============ Split datasets ============\")\n",
    "\n",
    "seed = 666                    # Fix random seed for reproducibility\n",
    "# Shuffle and split the data into train and a concatenation of validation and test sets\n",
    "..., ..., ..., .... = train_test_split(..., ...,      #TODO! \n",
    "                                       test_size=..., #TODO!\n",
    "                                       shuffle=True, \n",
    "                                       random_state=seed)\n",
    "\n",
    "seed = 221\n",
    "# Shuffle and split the data into validation and test sets\n",
    "..., ..., ..., .... = train_test_split(..., ...,      #TODO! \n",
    "                                       test_size=..., #TODO!\n",
    "                                       shuffle=True, \n",
    "                                       random_state=seed)                                                                   \n",
    "\n",
    "# sort with respect to x_axis for better plots\n",
    "X01_train, Y01_train = sort_wrt_x_axis(X01_train, Y01_train)\n",
    "X01_val, Y01_val = sort_wrt_x_axis(X01_val, Y01_val)\n",
    "X01_test, Y01_test = sort_wrt_x_axis(X01_test, Y01_test)\n",
    "\n",
    "\n",
    "# Store number of datapoints in each dataset:\n",
    "N_train = len(Y01_train)\n",
    "N_val = ..., #TODO!\n",
    "N_test = ..., #TODO!\n",
    "print(\"Datapoints used for training:   \", N_train)\n",
    "print(\"Datapoints used for validation: \", N_val)\n",
    "print(\"Datapoints used for testing :   \", N_test)\n",
    "\n",
    "print(\" ============ Using Formula ============\")\n",
    "print(\" --- Fit the linear model ---\")\n",
    "# ----------------------\n",
    "# Training\n",
    "# ----------------------\n",
    "... = fit_LR(..., ..., #TODO!\n",
    "             add_biais=True)\n",
    "print(\"W_0 : %.2f\" %W[0])\n",
    "print(\"W_1 : %.2f\" %W[1])\n",
    "# Evaluating performance on training data\n",
    "Y01_pred = predict_LR(X01_train, W, add_biais=True)\n",
    "print(\"Mean Square Error: %.2f\" %mean_squared_error(Y01_train, Y01_pred))\n",
    "print('Coefficient of determination: %.2f' %r2_score(Y01_train, Y01_pred))\n",
    "# ----------------------\n",
    "# Plots\n",
    "# ----------------------\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.scatter(X01_train,Y01_train, label=\"Training data\")\n",
    "plt.plot(X01_train,Y01_pred, c=\"r\", marker=\".\", label=\"Linear regression\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\" --- Predictions ---\")\n",
    "# ----------------------\n",
    "# Evaluation\n",
    "# ----------------------\n",
    "... = predict_LR(..., ..., #TODO!\n",
    "                 add_biais=True)\n",
    "# Performance on validation data\n",
    "print(\"Mean Square Error: %.2f\" %mean_squared_error(Y01_val, Y01_pred))\n",
    "print('Coefficient of determination: %.2f' %r2_score(Y01_val, Y01_pred))\n",
    "\n",
    "\n",
    "print(\" ============ Using scikit-learn ============ \")\n",
    "regr = LinearRegression()\n",
    "print(\" --- Fit the linear model ---\")\n",
    "# ----------------------\n",
    "# Training\n",
    "# ----------------------\n",
    "# Reshape your data either using array.reshape(-1, 1) if your data has a single feature\n",
    "regr.fit(..., ...) #TODO!\n",
    "print(\"W_0 : %.2f\" %regr.intercept_[0])\n",
    "print(\"W_1 : %.2f\" %regr.coef_[0,0])\n",
    "\n",
    "print(\" --- Predictions ---\")\n",
    "# Reshape your data either using array.reshape(-1, 1) if your data has a single feature\n",
    "Y01_pred_scikit = regr.predict(...)              #TODO!\n",
    "print(\"Mean Square Error: %.2f\" %...)            #TODO!\n",
    "print('Coefficient of determination: %.2f' %...) #TODO!\n",
    "# ----------------------\n",
    "# Plots\n",
    "# ----------------------\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.scatter(X01_train,Y01_train, label=\"Training data\")\n",
    "plt.scatter(X01_val,Y01_val, label=\"Expected values\")\n",
    "plt.plot(X01_val,Y01_pred, c=\"r\", ls=\"--\",lw=1, label=\"Linear regression\")\n",
    "plt.plot(X01_val, Y01_pred_scikit, c=\"g\", ls=\":\", lw=0.5, label=\"Scikit-learn model\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple linear regression on non-linear data\n",
    "\n",
    "1. Datasets \n",
    "  1. Using the function [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) from scikit-learn library, split ``X02`` and ``Y02`` into 3 datasets, a training one, a validation one and a testing one with a ratio 0.6, 0.2, 0.2.\n",
    "2. Fitting your model\n",
    "  1. Call ``fit_LR`` on your training dataset to compute the weights (store them in ``W``)\n",
    "3. Predictions\n",
    "  1. Call  ``predict_LR`` on your validation dataset and store the output in ``Y02_pred``\n",
    "  2. Compute the Mean Squared Error (MSE) of ``Y02_pred``\n",
    "  3. Computed the coefficient of determination $R^2$ of ``Y02_pred``\n",
    "4. Interpretation\n",
    "  1. Do you think that the linear regression works well here? Why?\n",
    "  2. If you had to compare the performance of this linear regression with the performance of the linear regression in the cell above, would you rather look at the mean squared error or the coefficient of determination? Why?\n",
    "  3. Would a bigger training dataset improve the perfomance? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" ============ Split datasets ============\")\n",
    "\n",
    "seed = 666                    # Fix random seed for reproducibility\n",
    "# Shuffle and split the data into train and a concatenation of validation and test sets\n",
    "X02_train, X02_val_test, Y02_train, Y02_val_test = ... #TODO!\n",
    "seed = 221\n",
    "# Shuffle and split the data into validation and test sets\n",
    "X02_val, X02_test, Y02_val, Y02_test = ...             #TODO!\n",
    "\n",
    "# sort with respect to x_axis for better plots\n",
    "X02_train, Y02_train = sort_wrt_x_axis(X02_train, Y02_train)\n",
    "X02_val, Y02_val = sort_wrt_x_axis(X02_val, Y02_val)\n",
    "X02_test, Y02_test = sort_wrt_x_axis(X02_test, Y02_test)\n",
    "print(\"Datapoints used for training:   \", len(Y02_train))\n",
    "print(\"Datapoints used for validation: \", len(Y02_val))\n",
    "print(\"Datapoints used for testing :   \", len(Y02_test))\n",
    "\n",
    "print(\" ============ Linear regression ============\")\n",
    "# ----------------------\n",
    "# Training\n",
    "# ----------------------\n",
    "... = fit_LR(..., ...)             #TODO!\n",
    "print(\"W_0 : %.2f\" %W[0])\n",
    "print(\"W_1 : %.2f\" %W[1])\n",
    "# ----------------------\n",
    "# Evaluation\n",
    "# ----------------------\n",
    "... = predict_LR(...)                            #TODO!\n",
    "print(\"Mean Square Error: %.2f\" %...)            #TODO!\n",
    "print('Coefficient of determination: %.2f' %...) #TODO!\n",
    "# ----------------------\n",
    "# Plots\n",
    "# ----------------------\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(X02_train,Y02_train, label=\"Training data\")\n",
    "plt.scatter(X02_val,Y02_val, label=\"Expected values\")\n",
    "plt.plot(X02_val,Y02_pred, c=\"r\", marker=\".\", label=\"Linear regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression with polynomial basis functions\n",
    "\n",
    "From now on we will apply a non-linear transformation to ``X``, store the transformed data in a matrix ``phi`` and then  perform linear regression. \n",
    "This is called a linear basis functions model. \n",
    "\n",
    "In the lecture (slide 18), there is an example with a polynomial basis function $phi(x) = (1, x, x^2)$\n",
    "\n",
    "Questions:\n",
    "\n",
    "1. Function ``phi_polynomial``:\n",
    "  1. complete this function taking the data ``X`` and a max degree ``d`` and returns ``phi`` containing the following matrix \n",
    "\\begin{equation}\n",
    "\\Phi = \n",
    "\\begin{bmatrix}\n",
    "1 & x_{1} & x_{1}^2 & \\cdots & x_{1}^d \\\\\n",
    "1 & x_{2} & x_{2}^2 & \\cdots & x_{2}^d \\\\\n",
    "\\vdots & \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "1 & x_{n} & x_{n}^2 & \\cdots & x_{n}^d\n",
    "\\end{bmatrix} \\in R^{(n, d+1)}\n",
    "\\end{equation}\n",
    "2. For all ``d`` in ``d_list``:\n",
    "  1. Training:\n",
    "    1. Compute ``phi_train`` by calling  ``phi_polynomial`` on your training data\n",
    "    2. Compute ``W`` by calling ``fit_LR`` on your transformed data and your training set of  expected values\n",
    "  2. Evaluation:\n",
    "    1. Transform your validation data and store them in ``phi_val``\n",
    "    2. Compute your predictions ``Y02_pred`` given your transformed validation data and your weights\n",
    "    3. Look at how the mse and the coefficient of determination $R^2$ were computed with the training data. Do the same with your validation data and store the mse and $R^2$ in ``mse_val`` and ``r2_val``\n",
    "3. Interpretation:\n",
    "  1. What do you obtain for ``d=0``?\n",
    "  2. What do you obtain for ``d=1``? Compare with the cell above.\n",
    "  3. By looking at the mse and coefficient of determination, which degree would you choose for your model?\n",
    "  4. Models with a degree between 5 and 20 seem to work similarly. In the situation when several models of different complexity have approximately the same performance,  what is usually the preferred option?\n",
    "  5. For which values of ``d`` does the model seem to overfit?\n",
    "  6. For ``d>16`` the performance on training data is getting worse. According to you, what could cause that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------\n",
    "# Polynomial basis functions\n",
    "# ----------------------\n",
    "def phi_polynomial(X,d):\n",
    "    \"\"\"\n",
    "    Compute polynomial basis functions\n",
    "    \n",
    "    The biais is included in this function\n",
    "    \"\"\"\n",
    "    # TODO!\n",
    "    return phi\n",
    "\n",
    "print(\" ============ Linear regression with polynomial basis functions ============\")\n",
    "mse_train = []\n",
    "mse_val = []\n",
    "r2_train = []\n",
    "r2_val= []\n",
    "d_list = list(range(6))+list(range(6,35,5))\n",
    "for d in d_list:\n",
    "    # ----------------------\n",
    "    # Training\n",
    "    # ----------------------\n",
    "    phi_train = phi_polynomial(...)  # TODO!\n",
    "    W = fit_LR(..., ...,\n",
    "               add_biais=False)  #The biais is already included in phi\n",
    "    # ----------------------\n",
    "    # Evaluation\n",
    "    # ----------------------\n",
    "    # Evaluate performance on training data\n",
    "    Y02_pred_train = predict_LR(phi_train,W, add_biais=False) #The biais is already included in phi\n",
    "    mse_train.append(mean_squared_error(Y02_train, Y02_pred_train))\n",
    "    r2_train.append(r2_score(Y02_train, Y02_pred_train))\n",
    "    \n",
    "    # Evaluate performance on validation data\n",
    "    phi_val = phi_polynomial(...) # TODO!\n",
    "    Y02_pred = predict_LR(... , ...,       # TODO!\n",
    "                          add_biais=False) #The biais is already included in phi\n",
    "    mse_val.append(...)    # TODO!\n",
    "    r2_val.append(...)     # TODO!\n",
    "    # ----------------------\n",
    "    # Plots\n",
    "    # ----------------------\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.scatter(X02_train,Y02_train, label=\"Training data\")\n",
    "    plt.scatter(X02_val,Y02_val, label=\"Expected values\")\n",
    "    plt.plot(X02_val,Y02_pred, c=\"r\", marker=\".\", label=\"degree d=\"+str(d))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# ----------------------\n",
    "# Overview\n",
    "# ----------------------\n",
    "def plot_mse_r2(x_axis, mse_train, mse_val, r2_train, r2_val):\n",
    "    \"\"\"\n",
    "    Plot training and validation Mean Squared Error and r2  \n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.ylim(min(min(mse_train),min(mse_val)), min(max(max(mse_train), max(mse_val)), 300))\n",
    "    plt.plot(x_axis, mse_train, label=\"Training\")\n",
    "    plt.plot(x_axis, mse_val, label=\"Validation\")\n",
    "    plt.legend()\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.ylim(0,1)\n",
    "    plt.plot(x_axis, r2_train, label=\"Training\")\n",
    "    plt.plot(x_axis, r2_val, label=\"Validation\")\n",
    "    plt.legend()\n",
    "    \n",
    "plot_mse_r2(d_list, mse_train, mse_val, r2_train, r2_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression with polynomial basis functions on non-polynomial data\n",
    "\n",
    "1. Datasets \n",
    "  1. Using the function [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) from scikit-learn library, split ``X03`` and ``Y03`` into 3 datasets, a training one, a validation one and a testing one with a ratio 0.6, 0.2, 0.2.\n",
    "2. Polynomial basis functions using scikit-learn:\n",
    " 1. Apply the [fit()](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.fit) method to ``poly_model``  using your training dataset.\n",
    "  2. Apply the [predict()](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.predict) method to ``poly_model`` using your validation dataset.\n",
    "  3. Compute the coefficient of determination $R^2$ of your predictions on your validation data and store the successive values in ``mse_val`` and ``r2_val``\n",
    "3. Interpretation:\n",
    "  1. Does the model work well?\n",
    "  2. Do you think polynomial functions are adapted for this problem?\n",
    "  3. For ``d>20`` the performance on training data is getting worse. According to you, what could cause that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" ============ Split datasets ============\")\n",
    "\n",
    "seed = 666                    # Fix random seed for reproducibility\n",
    "# Shuffle and split the data into train and a concatenation of validation and test sets\n",
    "... # TODO!\n",
    "seed = 221\n",
    "# Shuffle and split the data into validation and test sets\n",
    "... # TODO!\n",
    "\n",
    "# sort with respect to x_axis for better plots\n",
    "X03_train, Y03_train = sort_wrt_x_axis(X03_train, Y03_train)\n",
    "X03_val, Y03_val = sort_wrt_x_axis(X03_val, Y03_val)\n",
    "X03_test, Y03_test = sort_wrt_x_axis(X03_test, Y03_test)\n",
    "print(\"Datapoints used for training:   \", len(Y03_train))\n",
    "print(\"Datapoints used for validation: \", len(Y03_val))\n",
    "print(\"Datapoints used for testing :   \", len(Y03_test))\n",
    "\n",
    "print(\" ============ Using scikit learn ============\")\n",
    "mse_train = []\n",
    "mse_val = []\n",
    "r2_train = []\n",
    "r2_val= []\n",
    "d_list = list(range(2,10))+list(range(10,30,5))\n",
    "for d in d_list:\n",
    "    poly_model = make_pipeline(PolynomialFeatures(d),\n",
    "                               LinearRegression())\n",
    "    # ----------------------\n",
    "    # Training\n",
    "    # ----------------------\n",
    "    # Use array.reshape(-1, 1) if your data has a single feature\n",
    "    poly_model.fit(..., ...) # TODO!\n",
    "    # ----------------------\n",
    "    # Evaluation\n",
    "    # ----------------------\n",
    "    # Evaluate performance on training data\n",
    "    # Use array.reshape(-1, 1) if your data has a single feature\n",
    "    y_pred_train = poly_model.predict(X03_train.reshape(-1, 1))\n",
    "    mse_train.append(mean_squared_error(Y03_train, y_pred_train))\n",
    "    r2_train.append(r2_score(Y03_train, y_pred_train))\n",
    "    \n",
    "    # Evaluate performance on validation data\n",
    "    y_pred_scikit = poly_model.predict(...)  # TODO!\n",
    "    mse_val.append(...)                      # TODO!\n",
    "    r2_val.append(...)                       # TODO!\n",
    "    # ----------------------\n",
    "    # Plots\n",
    "    # ----------------------\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.scatter(X03_train,Y03_train, label=\"Training data\")\n",
    "    plt.scatter(X03_val,Y03_val, label=\"Expected values\")\n",
    "    plt.plot(X03_val,y_pred_scikit, c=\"r\", marker=\".\", label=\"Scikit-learn, degree d=\"+str(d))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "# ----------------------\n",
    "# Overview\n",
    "# ----------------------\n",
    "plot_mse_r2(d_list, mse_train, mse_val, r2_train, r2_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine regression\n",
    "\n",
    "Since the polynomial basis functions did not seem to work well on these data, we will use another type of basis function: cosine functions. To do so we need to implement a new function ``phi_cosine`` that will transform our data ``X`` using cosine functions.\n",
    "Questions:\n",
    "\n",
    "1. Function ``phi_cosine``:\n",
    "  1. Using [np.cos()](https://numpy.org/doc/stable/reference/generated/numpy.cos.html#numpy-cos), complete this function taking the data ``X`` and a number of cosine ``n_cosine`` and returns ``phi`` containing the following matrix \n",
    "\\begin{equation}\n",
    "\\Phi = \n",
    "\\begin{bmatrix}\n",
    "1 & cos(x_{1}) & cos(2   x_{1}) & \\cdots & cos(n_{cosine}   x_{1}) \\\\\n",
    "1 & cos(x_{2}) & cos(2   x_{2}) & \\cdots & cos(n_{cosine}   x_{2}) \\\\\n",
    "\\vdots & \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "1 & cos(x_{n}) & cos(2   x_{n}) & \\cdots & cos(n_{cosine}   x_{n})\n",
    "\\end{bmatrix} \\in R^{(n, n_{cosine}+1)}\n",
    "\\end{equation}\n",
    "2. Training\n",
    "  1. Transform and fit your model using ``phi_cosine`` and ``fit_LR`` (Hint: look at what  you did with with polynomial functions)\n",
    "3. Evaluation\n",
    "  1. Use ``predict_LR`` on your training and validation datasets. \n",
    "  2. Compute and store your mean square errors and coefficient of determination in ``mse_train, mse_val, r2_train, r2_val``\n",
    "4. Interpretation\n",
    "  1. Does a cosine transformation seem to work better than a polynomial transformation on this data? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_cosine(X,n_cosine):\n",
    "    # TODO!\n",
    "    return phi\n",
    "\n",
    "mse_train = []\n",
    "mse_val = []\n",
    "r2_train = []\n",
    "r2_val= []\n",
    "n_list = list(range(1,10))\n",
    "for n in n_list:\n",
    "    # ----------------------\n",
    "    # Training\n",
    "    # ----------------------\n",
    "    phi_train = phi_cosine(...)   # TODO!\n",
    "    W = fit_LR(..., ..., ...)     # TODO!\n",
    "    # ----------------------\n",
    "    # Evaluation\n",
    "    # ----------------------\n",
    "    # Evaluate performance on training data\n",
    "    Y03_pred_train = predict_LR(..., ..., ...)  # TODO!\n",
    "    mse_train.append(...)                       # TODO!\n",
    "    r2_train.append(...)                        # TODO!\n",
    "    \n",
    "    # Evaluate performance on validation data\n",
    "    phi_val = phi_cosine(...)                   # TODO!\n",
    "    Y03_pred = predict_LR(..., ..., ...)        # TODO!\n",
    "    mse_val.append(...)                         # TODO!\n",
    "    r2_val.append(...)                          # TODO!\n",
    "    # ----------------------\n",
    "    # Plots\n",
    "    # ----------------------\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.scatter(X03_train,Y03_train, label=\"Training data\")\n",
    "    plt.scatter(X03_val,Y03_val, label=\"Expected values\")\n",
    "    plt.plot(X03_val,Y03_pred, c=\"r\", marker=\".\", label=n)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "# ----------------------\n",
    "# Overview\n",
    "# ----------------------\n",
    "plot_mse_r2(n_list, mse_train, mse_val, r2_train, r2_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian regression\n",
    "\n",
    "\n",
    "In the lecture (slide 19), there is an example with a gaussian basis function $$\\Phi(x) = (e^{\\epsilon(||\\mathbf{x}-\\mathbf{c}||)})$$\n",
    "\n",
    "Where $\\mathbf{c}$ is the center points\n",
    "\n",
    "In the cell below we implemented ``phi_gaussian`` that works exactly as ``phi_polynomial`` and ``phi_cosine`` work.\n",
    "\n",
    "Questions:\n",
    "\n",
    "There are no questions for this one :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_gaussian(X,n_gaussians, sigma=None):\n",
    "    \"\"\"\n",
    "    Gaussian basis functions, uniformly spaced\n",
    "    \n",
    "    The biais is included in this function\n",
    "    \"\"\"\n",
    "    phi = np.ones((len(X),n_gaussians+1))\n",
    "    centers = np.linspace(X.min(), X.max(), n_gaussians)\n",
    "    if sigma is None:\n",
    "        if n_gaussians < 2:\n",
    "            sigma = (X.max()-X.min())/2\n",
    "        else:\n",
    "            sigma = (centers[1]-centers[0])/2\n",
    "    for i in range(1,n_gaussians+1):\n",
    "        phi[:,i] = np.exp(-(X-centers[i-1])**2/sigma)\n",
    "    return phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the right basis functions and the right parameters\n",
    "\n",
    "In this notebook you could observe that the performance a given transformation (basis function) depends a lot on your problem (i.e your data) \n",
    "\n",
    "Then once the transformation is choosen, the complexity of your model (how many basis functions you have) influences the performance too. \n",
    "\n",
    "In this cell you will use a new dataset: ``X04, Y04``. You will need to preprocess it as we did for the 3 other datasets and you will have to choose only one model (i.e one type of basis function and the number of basis functions).\n",
    "\n",
    "You can choose among:\n",
    "\n",
    "- simple linear regression (no transformation)\n",
    "- polynomial basis functions\n",
    "- cosine basis functions\n",
    "- gaussian basis functions\n",
    "\n",
    "We encourage you to try them all to make sure you find the best one :) \n",
    "\n",
    "Then tune your model parameters (which are ``d``, ``n_cosine``, ``n_gaussians`` here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
